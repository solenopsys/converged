/* proxy-plugin.ts
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Elysia **plugin** which forwards /api/responses requests to
   OpenAI **Responses API** with full Serverâ€‘Sentâ€‘Events streaming.
   All user messages and every SSE event from OpenAI are forwarded
   to an injected LogService implementation, so the proxy itself
   remains stateless.
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

import { Elysia } from "elysia";
import { cors } from "@elysiajs/cors";
import OpenAI from "openai";
import { randomUUID } from "crypto";

import type { ContentBlock, LogService } from "./types";
import { LogServiceHub } from "./log/log-service-hub";
import { LogServiceConsole } from "./log/log-service-console";
import { LogServiceSQLite } from "./log/log-service-sqlite";

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.  DIâ€‘Ñ‚Ð¾Ñ‡ÐºÐ° (Ð¿Ð¾Ð´Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹) â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const log: LogService = new LogServiceHub();

// ðŸ‘‰ Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
log.add(new LogServiceConsole());
log.add(new LogServiceSQLite());

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.  ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const DEFAULT_MODEL = process.env.DEFAULT_MODEL ?? "gpt-4o";
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/* helper: Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° â†’ ContentBlock[] */
const toBlocks = (text: string): ContentBlock[] => [
	{ type: "text", text, annotations: [] },
];

/*
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 *  PLUGIN FACTORY
 *
 *  Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ:
 *    import { Elysia } from "elysia"
 *    import responsesProxy from "./proxy-plugin"
 *
 *    const app = new Elysia()
 *      .use(responsesProxy)
 *      .listen(3000)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */
export const responsesProxyPlugin = (app: Elysia) =>
	app
		.use(cors())

		/* 2.1  Streamingâ€‘proxy Ð´Ð»Ñ /api/responses */
		.post("/api/responses", async ({ body, set, request }) => {
			try {
				const {
					input,
					model = DEFAULT_MODEL,
					previous_response_id,
					truncation = "auto",
					...extra
				} = body as {
					input: { role: string; content: string }[];
					model?: string;
					previous_response_id?: string;
					truncation?: "auto" | "disabled";
					[k: string]: unknown;
				};

				if (!Array.isArray(input)) {
					set.status = 400;
					return { error: "'input' must be an array of messages" };
				}

				/* conversationId: Ð»Ð¸Ð±Ð¾ previous_response_id, Ð»Ð¸Ð±Ð¾ fresh uuid */
				const conversationId =
					typeof previous_response_id === "string"
						? previous_response_id
						: randomUUID();

				/* 2.1.1  Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð’Ð¡Ð• Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ */
				await Promise.all(
					input.map((m) =>
						log.saveUserMessage({
							conversationId,
							role: "user",
							content: toBlocks(m.content),
							model,
							timestamp: new Date(),
							meta: {
								ip: request.headers.get("x-forwarded-for") ?? undefined,
							},
						}),
					),
				);

				/* 2.1.2  Ð—Ð°Ð¿Ñ€Ð¾Ñ Ðº OpenAI ÑÐ¾ ÑÑ‚Ñ€Ð¸Ð¼Ð¸Ð½Ð³Ð¾Ð¼ */
				const stream = await openai.responses.create({
					model,
					input,
					stream: true,
					previous_response_id,
					truncation,
					...extra,
				});

				/* 2.1.3  SSEâ€‘Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ */
				set.headers = {
					"Content-Type": "text/event-stream",
					"Cache-Control": "no-cache",
					Connection: "keep-alive",
				};

				const encoder = new TextEncoder();

				return new ReadableStream({
					async start(controller) {
						for await (const ev of stream) {
							const type = (ev as any).type ?? "unknown";

							/* 1) Ð¾Ñ‚Ð´Ð°Ñ‘Ð¼ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñƒ */
							controller.enqueue(
								encoder.encode(
									`event: ${type}\n` + `data: ${JSON.stringify(ev)}\n\n`,
								),
							);

							/* 2) Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‘Ð¼ Ð² Ð»Ð¾Ð³â€‘ÑÐµÑ€Ð²Ð¸Ñ (fireâ€‘andâ€‘forget) */
							void log.saveOpenAIEvent({
								conversationId,
								model,
								event: {
									type,
									payload: ev,
									receivedAt: new Date(),
								},
							});

							/* Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ SSE Ð¿Ð¾ÑÐ»Ðµ ÑÐ¸Ð³Ð½Ð°Ð»Ð° completed/done */
							if (type === "response.completed" || type === "done") {
								controller.enqueue(
									encoder.encode(`event: done\ndata: [DONE]\n\n`),
								);
								controller.close();
							}
						}
					},
				});
			} catch (err: any) {
				set.status = 500;
				return { error: err?.message ?? "Internal Server Error" };
			}
		})

		/* 2.2  Helper: ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ realtimeâ€‘ÑÐµÑÑÐ¸ÑŽ OpenAI */
		.get("/api/session", async ({ set }) => {
			try {
				const r = await fetch("https://api.openai.com/v1/realtime/sessions", {
					method: "POST",
					headers: {
						Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
						"Content-Type": "application/json",
					},
					body: JSON.stringify({ model: DEFAULT_MODEL }),
				});
				return await r.json();
			} catch {
				set.status = 500;
				return { error: "Unable to create session" };
			}
		})

		/* 2.3  Healthâ€‘check */
		.get("/api", () => "OpenAI Responses proxy is up!");

export default responsesProxyPlugin;
